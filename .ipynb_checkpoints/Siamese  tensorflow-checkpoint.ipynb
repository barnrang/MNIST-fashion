{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper.load_batch as lb\n",
    "import helper.load_music as ld\n",
    "import pygame\n",
    "songs = ld.music_load(dir='Music_sample',length=8000)\n",
    "song_test = songs[0].reshape(-1,2)\n",
    "pygame.mixer.init(44100,-16,2)\n",
    "sound = pygame.sndarray.make_sound(song_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train, X_test = [np.reshape(_,(-1,28,28,1))/255. for _ in [X_train, X_test]]\n",
    "X_val, y_val = X_train[-5000:], y_train[-5000:]\n",
    "X_train, y_train = X_train[:-5000], y_train[:-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'X_train':X_train,\n",
    "    'X_val':X_val,\n",
    "    'X_test':X_test,\n",
    "    'y_train':y_train,\n",
    "    'y_val':y_val,\n",
    "    'y_test':y_test,\n",
    "    'num_class':10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初期値\n",
    "論文により、\n",
    "- Kernel for Convolution is normal distribution with mean 0 and standard deviation 0.01\n",
    "\n",
    "同様に以下の通りに初期値を与えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Winit = tf.random_normal_initializer(mean=0,stddev=0.01)\n",
    "binit = tf.random_normal_initializer(mean=0.5,stddev=0.01)\n",
    "Denseinit = tf.random_normal_initializer(mean=0,stddev=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as Before\n",
    "def model(X, reg_pow=0.001):\n",
    "    reg = tf.contrib.layers.l2_regularizer(scale=reg_pow)\n",
    "    con = tf.constant_initializer\n",
    "    with tf.variable_scope('conv1'):\n",
    "        X = tf.layers.conv2d(X,32,kernel_size=[2,2],strides=[1,1],activation=tf.nn.relu\n",
    "                     ,kernel_initializer=Winit,bias_initializer=binit,kernel_regularizer=reg)\n",
    "        X = tf.layers.max_pooling2d(X,[2,2],2)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        X = tf.layers.conv2d(X,128,kernel_size=[2,2],strides=[1,1],activation=tf.nn.relu\n",
    "                        ,kernel_initializer=Winit,bias_initializer=binit,kernel_regularizer=reg)\n",
    "        X = tf.layers.max_pooling2d(X,[2,2],2)\n",
    "        X = tf.contrib.layers.flatten(X)\n",
    "    with tf.variable_scope('full1'):\n",
    "        X = tf.layers.dense(X,512,kernel_initializer=Denseinit,kernel_regularizer=reg,activation=tf.sigmoid)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 距離\n",
    "$$ p = \\sigma\\left(\\sum_{j}\\alpha_j\\left|h_{1,L-1}^{(j)}-h_{1,L-1}^{(j)}\\right|\\right) $$\n",
    "実践的に dense layer 一層で用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_predict(X1,X2):\n",
    "    '''\n",
    "    Input\n",
    "    X1,X2: (N,M)\n",
    "    Return\n",
    "    logistic prediction\n",
    "    '''\n",
    "    dim = X1.get_shape()[1]\n",
    "    diff = tf.abs(X1-X2)\n",
    "    out = tf.layers.dense(diff,1,kernel_initializer=Denseinit, use_bias=False)\n",
    "    return tf.reduce_sum(diff,axis=1), out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X1 = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "X2 = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "y = tf.placeholder(tf.int32,[None,1])\n",
    "is_training = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('bottleneck') as scope:\n",
    "    Y1 = model(X1)\n",
    "    scope.reuse_variables()\n",
    "    Y2 = model(X2)\n",
    "dist, p = combine_predict(Y1,Y2)\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y,tf.float32),logits=p)) \\\n",
    "    + tf.reduce_sum(reg_losses)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "predict = tf.cast(tf.greater(p,0.), dtype=tf.int32)\n",
    "correct_pred = tf.equal(predict, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from siamese_model/siamese\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'siamese_model/siamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = lb.BatchLoader(**data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 28, 28, 1), (8, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1,t2,yt = data.make_batch(4)\n",
    "t1.shape, yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 0.24887996912002563 and accuracy is 0.9149999618530273\n",
      "val loss is 0.25411179661750793 and accuracy is 0.9210000038146973\n",
      "train loss is 0.2455081045627594 and accuracy is 0.9149999618530273\n",
      "val loss is 0.26558002829551697 and accuracy is 0.9039999842643738\n",
      "train loss is 0.22822628915309906 and accuracy is 0.9399999380111694\n",
      "val loss is 0.2257498949766159 and accuracy is 0.9390000700950623\n",
      "train loss is 0.37398648262023926 and accuracy is 0.8900001049041748\n",
      "val loss is 0.2609494924545288 and accuracy is 0.9070000052452087\n",
      "train loss is 0.28646329045295715 and accuracy is 0.9049999713897705\n",
      "val loss is 0.25355684757232666 and accuracy is 0.9109999537467957\n",
      "train loss is 0.22070737183094025 and accuracy is 0.9399999380111694\n",
      "val loss is 0.24210910499095917 and accuracy is 0.9240000247955322\n",
      "train loss is 0.27953603863716125 and accuracy is 0.9049999713897705\n",
      "val loss is 0.24856387078762054 and accuracy is 0.9179999828338623\n",
      "train loss is 0.26773956418037415 and accuracy is 0.8949999809265137\n",
      "val loss is 0.24126115441322327 and accuracy is 0.9200000166893005\n",
      "train loss is 0.2050073742866516 and accuracy is 0.9299999475479126\n",
      "val loss is 0.24711911380290985 and accuracy is 0.9230000376701355\n",
      "train loss is 0.2747848927974701 and accuracy is 0.9299999475479126\n",
      "val loss is 0.25832051038742065 and accuracy is 0.902999997138977\n",
      "train loss is 0.21168477833271027 and accuracy is 0.9299999475479126\n",
      "val loss is 0.23612511157989502 and accuracy is 0.9240000247955322\n",
      "train loss is 0.25273072719573975 and accuracy is 0.9249999523162842\n",
      "val loss is 0.2361806184053421 and accuracy is 0.9270000457763672\n",
      "train loss is 0.2124195098876953 and accuracy is 0.9249999523162842\n",
      "val loss is 0.24403564631938934 and accuracy is 0.9200000762939453\n",
      "train loss is 0.2258993238210678 and accuracy is 0.9149999618530273\n",
      "val loss is 0.24206334352493286 and accuracy is 0.9220000505447388\n",
      "train loss is 0.22856947779655457 and accuracy is 0.9300000071525574\n",
      "val loss is 0.238066628575325 and accuracy is 0.9220000505447388\n",
      "train loss is 0.20055246353149414 and accuracy is 0.9299999475479126\n",
      "val loss is 0.22496920824050903 and accuracy is 0.921000063419342\n",
      "train loss is 0.22539883852005005 and accuracy is 0.9249999523162842\n",
      "val loss is 0.22443372011184692 and accuracy is 0.9310000538825989\n",
      "train loss is 0.22104205191135406 and accuracy is 0.9299999475479126\n",
      "val loss is 0.23246924579143524 and accuracy is 0.9269999861717224\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ab968223871e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdat_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mexpect\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} loss is {} and accuracy is {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "val_size = 500\n",
    "for i in range(10000):\n",
    "    for phase in ['train','val']:\n",
    "        if phase == 'train':\n",
    "            step = [loss, accuracy, train_step]\n",
    "            first, second, expect = data.make_batch(batch_size, dat_type='train')\n",
    "        else:\n",
    "            step = [loss, accuracy, correct_pred]\n",
    "            first, second, expect = data.make_batch(val_size, dat_type='val')\n",
    "        feed = {X1:first,X2:second,y:expect}\n",
    "        current_loss, acc, _ = sess.run(step, feed_dict=feed)\n",
    "        if i % 100 == 0:\n",
    "            print('{} loss is {} and accuracy is {}'.format(phase, current_loss, acc))\n",
    "\n",
    "sound.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sound.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "path = saver.save(sess,'siamese_model/siamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8642"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.do_test_oneshot(sess, p,X1,X2,repeat=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chatbot]",
   "language": "python",
   "name": "conda-env-chatbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
