{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "\n",
    "SiameseNet Oneshot\n",
    "\n",
    "http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-47a3b72556dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_batch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_music\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msongs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Music_sample'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msong_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msongs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/barnrang/Documents/Github/MNIST-fashion/helper/load_music.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m def music_load(dir='../Music_sample',load_list=[],length=5000\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'"
     ]
    }
   ],
   "source": [
    "import helper.load_batch as lb\n",
    "import helper.load_music as ld\n",
    "import pygame\n",
    "songs = ld.music_load(dir='Music_sample',length=8000)\n",
    "song_test = songs[0].reshape(-1,2)\n",
    "pygame.mixer.init(44100,-16,2)\n",
    "sound = pygame.sndarray.make_sound(song_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/barnrang/anaconda3/envs/chatbot/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from tensorflow.contrib.image import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 1s 37us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 117s 4us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 36s 8us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train, X_test = [np.reshape(_,(-1,28,28,1))/255. for _ in [X_train, X_test]]\n",
    "X_val, y_val = X_train[-5000:], y_train[-5000:]\n",
    "X_train, y_train = X_train[:-5000], y_train[:-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'X_train':X_train,\n",
    "    'X_val':X_val,\n",
    "    'X_test':X_test,\n",
    "    'y_train':y_train,\n",
    "    'y_val':y_val,\n",
    "    'y_test':y_test,\n",
    "    'num_class':10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初期値\n",
    "論文により、\n",
    "- Kernel for Convolution is normal distribution with mean 0 and standard deviation 0.01\n",
    "\n",
    "同様に以下の通りに初期値を与えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Winit = tf.random_normal_initializer(mean=0,stddev=0.01)\n",
    "binit = tf.random_normal_initializer(mean=0.5,stddev=0.01)\n",
    "Denseinit = tf.random_normal_initializer(mean=0,stddev=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as Before\n",
    "def model(X, reg_pow=0.001):\n",
    "    reg = tf.contrib.layers.l2_regularizer(scale=reg_pow)\n",
    "    con = tf.constant_initializer\n",
    "    with tf.variable_scope('conv1'):\n",
    "        X = tf.layers.conv2d(X,32,kernel_size=[2,2],strides=[1,1],activation=tf.nn.relu\n",
    "                     ,kernel_initializer=Winit,bias_initializer=binit,kernel_regularizer=reg)\n",
    "        X = tf.layers.max_pooling2d(X,[2,2],2)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        X = tf.layers.conv2d(X,128,kernel_size=[2,2],strides=[1,1],activation=tf.nn.relu\n",
    "                        ,kernel_initializer=Winit,bias_initializer=binit,kernel_regularizer=reg)\n",
    "        X = tf.layers.max_pooling2d(X,[2,2],2)\n",
    "        X = tf.contrib.layers.flatten(X)\n",
    "    with tf.variable_scope('full1'):\n",
    "        X = tf.layers.dense(X,512,kernel_initializer=Denseinit,kernel_regularizer=reg,activation=tf.sigmoid)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 距離\n",
    "$$ p = \\sigma\\left(\\sum_{j}\\alpha_j\\left|h_{1,L-1}^{(j)}-h_{1,L-1}^{(j)}\\right|\\right) $$\n",
    "実践的に dense layer 一層で用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_predict(X1,X2):\n",
    "    '''\n",
    "    Input\n",
    "    X1,X2: (N,M)\n",
    "    Return\n",
    "    logistic prediction\n",
    "    '''\n",
    "    dim = X1.get_shape()[1]\n",
    "    diff = tf.abs(X1-X2)\n",
    "    out = tf.layers.dense(diff,1,kernel_initializer=Denseinit, use_bias=False)\n",
    "    return tf.reduce_sum(diff,axis=1), out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "https://www.mathworks.com/discovery/affine-transformation.html\n",
    "\n",
    "T = (θ, ρx, ρy, sx, sy, tx, tx), with θ ∈　[−10.0, 10.0], ρx, ρy ∈ [−0.3, 0.3], sx, sy ∈ [0.8, 1.2],tx, ty ∈ [−2, 2]\n",
    "\n",
    "順番にrotation, shear, scale, translation\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\cos{\\theta} & \\sin{\\theta} \\\\\n",
    "-\\sin{\\theta} & \\cos{\\theta}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & \\rho_y \\\\\n",
    "\\rho_x & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_x & 0 \\\\\n",
    "0 & s_y\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "t_x\\\\\n",
    "t_y\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "a_0 & a_1 \\\\\n",
    "b_0 & b_1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "a_2\\\\\n",
    "b_2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(x):\n",
    "    return tf.less(tf.random_uniform([1]),x)[0]\n",
    "\n",
    "def affine_transform(X, rate):\n",
    "    trans_matrix = tf.eye(2)\n",
    "    def rotate():\n",
    "        theta = tf.random_uniform([1],-10,10)[0]/(180)*np.pi\n",
    "        sub_tran = tf.convert_to_tensor([[tf.cos(theta),tf.sin(theta)],[-tf.sin(theta), tf.cos(theta)]])\n",
    "        return tf.matmul(trans_matrix, sub_tran)\n",
    "    trans_matrix = tf.cond(prob(rate), rotate, lambda: trans_matrix)\n",
    "    def shear():\n",
    "        p = tf.random_uniform([2],-0.1,0.1)\n",
    "        sub_tran = tf.convert_to_tensor([[1,p[1]],[p[0], 1]])\n",
    "        return tf.matmul(trans_matrix, sub_tran)\n",
    "    trans_matrix = tf.cond(prob(rate), shear, lambda: trans_matrix)\n",
    "    def scale():   \n",
    "        s = tf.random_uniform([2],0.8,1.2)\n",
    "        sub_tran = tf.convert_to_tensor([[s[0],0],[0,s[1]]])\n",
    "        return tf.matmul(trans_matrix, sub_tran)\n",
    "    trans_matrix = tf.cond(prob(rate), scale, lambda: trans_matrix)\n",
    "    def translation():\n",
    "        return tf.random_uniform([2],-1,1)\n",
    "    t = tf.cond(prob(rate), translation, lambda: tf.zeros(2))\n",
    "    a0,a1,b0,b1 = trans_matrix[0][0],trans_matrix[0][1],trans_matrix[1][0],trans_matrix[1][1]\n",
    "    a2,b2 = t[0],t[1]\n",
    "    return transform(X, [a0,a1,a2,b0,b1,b2,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X1 = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "X2 = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "y = tf.placeholder(tf.int32,[None,1])\n",
    "is_training = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL,XR = tf.cond(is_training,\n",
    "                lambda:(affine_transform(X1,0.5),affine_transform(X2,0.5)),\n",
    "                lambda: (X1,X2))\n",
    "with tf.variable_scope('bottleneck') as scope:\n",
    "    Y1 = model(XL)\n",
    "    scope.reuse_variables()\n",
    "    Y2 = model(XR)\n",
    "dist, p = combine_predict(Y1,Y2)\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y,tf.float32),logits=p)) \\\n",
    "    + tf.reduce_sum(reg_losses)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "predict = tf.cast(tf.greater(p,0.), dtype=tf.int32)\n",
    "correct_pred = tf.equal(predict, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from siamese_model/siamese\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "try:\n",
    "    saver.restore(sess,'siamese_model/siamese')\n",
    "except:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = lb.BatchLoader(**data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe9c8479cf8>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAADDCAYAAAAr1dPWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERKZQmGyIiIlKo/wdNtMRn37Q39AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9c860c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "affine_test = X_train[3:4]\n",
    "trans_image = sess.run(XL, feed_dict={X1:affine_test,is_training:True})\n",
    "fig, axes = plt.subplots(1,3,figsize=(9,5))\n",
    "axes[0].imshow(affine_test[0,:,:,0])\n",
    "axes[0].set_title('original')\n",
    "axes[1].imshow(trans_image[0,:,:,0])\n",
    "axes[1].set_title('affine1')\n",
    "trans_image = sess.run(XL, feed_dict={X1:affine_test,is_training:False})\n",
    "axes[2].imshow(trans_image[0,:,:,0])\n",
    "axes[2].set_title('set no affine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 0.24887996912002563 and accuracy is 0.9149999618530273\n",
      "val loss is 0.25411179661750793 and accuracy is 0.9210000038146973\n",
      "train loss is 0.2455081045627594 and accuracy is 0.9149999618530273\n",
      "val loss is 0.26558002829551697 and accuracy is 0.9039999842643738\n",
      "train loss is 0.22822628915309906 and accuracy is 0.9399999380111694\n",
      "val loss is 0.2257498949766159 and accuracy is 0.9390000700950623\n",
      "train loss is 0.37398648262023926 and accuracy is 0.8900001049041748\n",
      "val loss is 0.2609494924545288 and accuracy is 0.9070000052452087\n",
      "train loss is 0.28646329045295715 and accuracy is 0.9049999713897705\n",
      "val loss is 0.25355684757232666 and accuracy is 0.9109999537467957\n",
      "train loss is 0.22070737183094025 and accuracy is 0.9399999380111694\n",
      "val loss is 0.24210910499095917 and accuracy is 0.9240000247955322\n",
      "train loss is 0.27953603863716125 and accuracy is 0.9049999713897705\n",
      "val loss is 0.24856387078762054 and accuracy is 0.9179999828338623\n",
      "train loss is 0.26773956418037415 and accuracy is 0.8949999809265137\n",
      "val loss is 0.24126115441322327 and accuracy is 0.9200000166893005\n",
      "train loss is 0.2050073742866516 and accuracy is 0.9299999475479126\n",
      "val loss is 0.24711911380290985 and accuracy is 0.9230000376701355\n",
      "train loss is 0.2747848927974701 and accuracy is 0.9299999475479126\n",
      "val loss is 0.25832051038742065 and accuracy is 0.902999997138977\n",
      "train loss is 0.21168477833271027 and accuracy is 0.9299999475479126\n",
      "val loss is 0.23612511157989502 and accuracy is 0.9240000247955322\n",
      "train loss is 0.25273072719573975 and accuracy is 0.9249999523162842\n",
      "val loss is 0.2361806184053421 and accuracy is 0.9270000457763672\n",
      "train loss is 0.2124195098876953 and accuracy is 0.9249999523162842\n",
      "val loss is 0.24403564631938934 and accuracy is 0.9200000762939453\n",
      "train loss is 0.2258993238210678 and accuracy is 0.9149999618530273\n",
      "val loss is 0.24206334352493286 and accuracy is 0.9220000505447388\n",
      "train loss is 0.22856947779655457 and accuracy is 0.9300000071525574\n",
      "val loss is 0.238066628575325 and accuracy is 0.9220000505447388\n",
      "train loss is 0.20055246353149414 and accuracy is 0.9299999475479126\n",
      "val loss is 0.22496920824050903 and accuracy is 0.921000063419342\n",
      "train loss is 0.22539883852005005 and accuracy is 0.9249999523162842\n",
      "val loss is 0.22443372011184692 and accuracy is 0.9310000538825989\n",
      "train loss is 0.22104205191135406 and accuracy is 0.9299999475479126\n",
      "val loss is 0.23246924579143524 and accuracy is 0.9269999861717224\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ab968223871e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdat_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mexpect\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} loss is {} and accuracy is {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "val_size = 500\n",
    "for i in range(10000):\n",
    "    for phase in ['train','val']:\n",
    "        if phase == 'train':\n",
    "            step = [loss, accuracy, train_step]\n",
    "            first, second, expect = data.make_batch(batch_size, dat_type='train')\n",
    "        else:\n",
    "            step = [loss, accuracy, correct_pred]\n",
    "            first, second, expect = data.make_batch(val_size, dat_type='val')\n",
    "        feed = {X1:first,X2:second,y:expect}\n",
    "        current_loss, acc, _ = sess.run(step, feed_dict=feed)\n",
    "        if i % 100 == 0:\n",
    "            print('{} loss is {} and accuracy is {}'.format(phase, current_loss, acc))\n",
    "\n",
    "sound.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sound.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "path = saver.save(sess,'siamese_model/siamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8642"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.do_test_oneshot(sess, p,X1,X2,repeat=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
